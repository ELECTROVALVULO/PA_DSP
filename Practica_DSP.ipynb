{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias basicas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar los datos\n",
    "dataset = pd.read_csv('Wine.csv')\n",
    "\n",
    "# Division en features y target\n",
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "features_numericas = dataset.iloc[:, 0:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar datos faltantes\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba que no existen datos faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cardinalidad del target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobacion cardinalidad del target\n",
    "target = dataset.Customer_Segment\n",
    "target = target.to_frame(name='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target:\n",
    "    print ('\\nLa cantidad de observaciones del target expresado en valor numérico es la siguiente:')\n",
    "    print (target[col].value_counts())\n",
    "    print ('\\nEl porcentaje de observaciones de cada categoría del target con respecto al total expresado en % es el siguiente:')\n",
    "    print (round(target[col].value_counts() / len(target[col])*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representacion grafica de la cardinalidad del target\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Customer_Segment\", data=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Comprobar existencia de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar existencia de outliers\n",
    "# Cálculo de los valores inferiores , superiores y sus porcentajes, coeficiente de 1,5\n",
    "# Cálculo de los valores extremos inferiores ,superiores y sus porcentajes, coeficiente de 3.\n",
    "# Creación de dataframe con los resultados.\n",
    "\n",
    "Columnas = []\n",
    "LimiteInferior = []\n",
    "LimiteSuperior = []\n",
    "LimiteInferiorPor = []\n",
    "LimiteSuperiorPorc = []\n",
    "LimiteExInferior = []\n",
    "LimiteExtSuperior = []\n",
    "LimiteExtInferiorPor = []\n",
    "LimiteExtSuperiorPorc = []\n",
    "\n",
    "for (index, colname) in  enumerate(features_numericas):    \n",
    "    \n",
    "    IQR = features_numericas[colname].quantile(0.75) - features_numericas[colname].quantile(0.25)\n",
    "    limSup = features_numericas[colname].quantile(0.75) + (IQR *  1.5)\n",
    "    limInf = features_numericas[colname].quantile(0.25) - (IQR *  1.5)\n",
    "    limSupExt = features_numericas[colname].quantile(0.75) + (IQR *  3)\n",
    "    limInfExt = features_numericas[colname].quantile(0.25) - (IQR *  3)\n",
    "    total = features_numericas[colname].shape[0]\n",
    "    limSupPorc = (round(features_numericas[features_numericas[colname] > limSup].shape[0]/total,2))*100\n",
    "    limInfPorc = (round(features_numericas[features_numericas[colname] < limInf].shape[0]/total,2))*100\n",
    "    limSupExtPorc = (round(features_numericas[features_numericas[colname] > limSupExt].shape[0]/total,2))*100\n",
    "    limInfExtPorc = (round(features_numericas[features_numericas[colname] < limInfExt].shape[0]/total,2))*100\n",
    "    \n",
    "    Columnas.append(colname)\n",
    "    LimiteInferior.append(limInf)\n",
    "    LimiteSuperior.append(limSup)\n",
    "    LimiteInferiorPor.append(limInfPorc)\n",
    "    LimiteSuperiorPorc.append(limSupPorc)\n",
    "    LimiteExInferior.append(limInfExt)\n",
    "    LimiteExtSuperior.append(limSupExt)\n",
    "    LimiteExtInferiorPor.append(limInfExtPorc)\n",
    "    LimiteExtSuperiorPorc.append(limSupExtPorc)\n",
    "    df_outliers = pd.DataFrame(list(zip(Columnas,LimiteInferior, LimiteInferiorPor ,LimiteSuperior, LimiteSuperiorPorc, LimiteExInferior , LimiteExtInferiorPor, LimiteExtSuperior, LimiteExtSuperiorPorc)), \n",
    "                             columns = ['Feature','Valor límite inferior','Valor límite inferior %', 'Valor límite superior','Valor límite superior %', 'Valor extremo inferior','Valor extremo inferior %',\n",
    "                                       'Valor extremo superior','Valor extremo superior %'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se observan valores outliers con representación relevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Distribución de las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuciones de las features\n",
    "features_numericas.hist(bins =20 ,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "# Datos numéricos\n",
    "a = 4 # numero filas\n",
    "b = 4  # numero columnas\n",
    "c = 1  # contador plots\n",
    "\n",
    "fig = plt.figure(figsize=(24,16))\n",
    "\n",
    "for i in features_numericas:\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.title('{}'.format(i))\n",
    "    plt.xlabel(i)\n",
    "    sns.boxplot(y=features_numericas[i],palette=\"Blues\", orient=\"v\")\n",
    "    c = c + 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(features_numericas, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Comprobación de la normalidad de las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comproba la normalidad del conjunto de features numéricas, se aplica el test estadísitico Shapiro Wilk\n",
    "#from scipy.stats import shapiro\n",
    "\n",
    "alpha = 0.05\n",
    "Variable = []\n",
    "Est = []\n",
    "Pval = []\n",
    "Resul = []\n",
    "for i in features_numericas:\n",
    "    #print (\"\\nFeature\" ,[i])\n",
    "    a,b = stats.shapiro(features_numericas[[i]])\n",
    "    #print( \"Estadístico\", a , \"p-valor\" , b)\n",
    "    if b < alpha:\n",
    "        \n",
    "        Variable.append(i)\n",
    "        Est.append(a)\n",
    "        Pval.append(b)\n",
    "        Resul.append(\"Se rechaza la hipótesis nula, la muestra no proviene de una distribución normal\")\n",
    "    else:\n",
    "        Variable.append([i])\n",
    "        Est.append(a)\n",
    "        Pval.append(b)\n",
    "        Resul.append(\"Se acepta la hipótesis nula, la muestra proviene de una distribución normal\")\n",
    "        \n",
    "    df_Normalidad = pd.DataFrame(list(zip(Variable,Est, Pval ,Resul)), \n",
    "                             columns = [\"Fature\", \"Estadístico\", \"P-valor\", \"Resultado\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "df_Normalidad[:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Comprobación de correlación entre las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "## Corresponde aplicar spearman pues segun el test de normalidad las features (con la excepcion de Ash_Alcanity) no son normales.\n",
    "corr_matrix = dataset.iloc[:, 0:13].corr(method='spearman')\n",
    "corr_matrix = round(corr_matrix ,3)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,10))\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True,\n",
    "            cbar_kws={\"label\": \"Correlation coefficient\", \"shrink\": 1.25, \"ticks\": np.linspace(-1.0, 1.0, 9)}, \n",
    "            cmap=plt.cm.hot_r, \n",
    "            vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función correlación\n",
    "def fun_corr_matrix(corr_mat):\n",
    "\n",
    "    corr_mat = corr_mat.stack().reset_index()\n",
    "    corr_mat.columns = ['variable_1','variable_2','r']\n",
    "    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n",
    "    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n",
    "    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n",
    "    \n",
    "    return(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla correlción\n",
    "corr_matrix = dataset.iloc[:, 0:13].corr(method='spearman')\n",
    "fun_corr_matrix(corr_matrix).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rangos de correlación propuestos:\n",
    "\n",
    "- Correlación < 0,3: correlación débil.\n",
    "- Correlación entre 0,3 y 0,7: correlación moderada\n",
    "- Correlación > 0,7: correlación fuerte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obervan fuertes correlaciones por lo que deberíamos el valorar la posibilidad de descartar algunas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el data set en conjunto de entrenamiento y conjunto de testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de variables\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47569261e7c0ea8a8534fcea6f14166a5e9ce0bf61fff476030c5752a23259c4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Tarea_PA_DSP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
